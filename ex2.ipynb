{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3975f5e274b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "\n",
    "\n",
    "def create_word_vec_dict():\n",
    "    vecs = np.loadtxt(\"pretrained vectors.txt\")\n",
    "    with open(\"words.txt\", 'r') as words_file:\n",
    "        words = words_file.read().splitlines()\n",
    "    words2vecs = dict()\n",
    "    words2inx = dict()\n",
    "    i, j = 0, 0\n",
    "    for word in words:\n",
    "        if word not in words2vecs:\n",
    "            words2inx[word] = j\n",
    "            vec = vecs[i]\n",
    "            words2vecs[word] = vec\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return words2vecs, words2inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_NER(file_path, window_size):\n",
    "    # initialize dictionaries\n",
    "    word2index = {'<S>': 0, '<E>': 1, '<U>': 2}\n",
    "    index2word = {0: '<S>', 1: '<E>', 2: '<U>'}\n",
    "    label2index = {'<START>': 0, '<END>': 1, '<UNSEEN>': 2}\n",
    "    index2label = {0: '<START>', 1: '<END>', 2: '<UNSEEN>'}\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        word_index = 3\n",
    "        label_index = 3\n",
    "        dataset = []\n",
    "\n",
    "        # split into sentences (separated by blank rows)\n",
    "        sentences = f.read().split('\\n\\n')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if sentence == '' or sentence == '\\n':\n",
    "                continue\n",
    "            # add special words of start and end of sentence with special labels (<S> = START, <E> = END)\n",
    "            sentence = '<S>\\tSTART\\n<S>\\tSTART\\n' + sentence + '\\n<E>\\tEND\\n<E>\\tEND'\n",
    "            words = sentence.split('\\n')\n",
    "\n",
    "            # go over the words (not including the start and end words)\n",
    "            for i in range(window_size, len(words) - window_size):\n",
    "                # for each word split into word and label\n",
    "                word, ner = words[i].split('\\t')\n",
    "\n",
    "                # insert to the dataset a tuple of label and 5 words when the label is of the middle word\n",
    "                dataset.append((ner, [word.split('\\t')[0] for word in words[i - window_size: i + window_size + 1]]))\n",
    "\n",
    "                # keep track of word and index\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = word_index\n",
    "                    index2word[word_index] = word\n",
    "                    word_index += 1\n",
    "\n",
    "                # keep track of label and index\n",
    "                if ner not in label2index:\n",
    "                    label2index[ner] = label_index\n",
    "                    index2label[label_index] = ner\n",
    "                    label_index += 1\n",
    "\n",
    "    return dataset, word2index, index2word, label2index, index2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_POS(file_path, window_size, pretrained=False):\n",
    "    # initialize dictionaries\n",
    "    word2index = {'<S>': 0, '<E>': 1, '<U>': 2}\n",
    "    index2word = {0: '<S>', 1: '<E>', 2: '<U>'}\n",
    "    label2index = {'<START>': 0, '<END>': 1, '<UNSEEN>': 2}\n",
    "    index2label = {0: '<START>', 1: '<END>', 2: '<UNSEEN>'}\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        word_index = 3\n",
    "        label_index = 3\n",
    "        dataset = []\n",
    "\n",
    "        # split into sentences (separated by blank rows)\n",
    "        sentences = f.read().split('\\n\\n')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if sentence == '' or sentence == '\\n':\n",
    "                continue\n",
    "            # add special words of start and end of sentence with special labels (<S> = START, <E> = END)\n",
    "            sentence = '<S> START\\n<S> START\\n' + sentence + '\\n<E> END\\n<E> END'\n",
    "            words = sentence.split('\\n')\n",
    "\n",
    "            # go over the words (not including the start and end words)\n",
    "            for i in range(window_size, len(words) - window_size):\n",
    "                # for each word split into word and label\n",
    "                word, pos = words[i].split(' ')\n",
    "\n",
    "                # insert to the dataset a tuple of label and 5 words when the label is of the middle word\n",
    "                dataset.append((pos, [word.split(' ')[0] for word in words[i - window_size: i + window_size + 1]]))\n",
    "\n",
    "                # keep track of word and index\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = word_index\n",
    "                    index2word[word_index] = word\n",
    "                    word_index += 1\n",
    "\n",
    "                # keep track of label and index\n",
    "                if pos not in label2index:\n",
    "                    label2index[pos] = label_index\n",
    "                    index2label[label_index] = pos\n",
    "                    label_index += 1\n",
    "\n",
    "    return dataset, word2index, index2word, label2index, index2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_index(dataset, word2index, label2index):\n",
    "    for i in range(len(dataset)):\n",
    "        # get current sample\n",
    "        pos, words = dataset[i]\n",
    "        # go over the words in the window\n",
    "        for j in range(len(words)):\n",
    "            # convert word to index. if the word was not seen - convert to unseen letter\n",
    "            dataset[i][1][j] = word2index.get(words[j], word2index['<U>'])\n",
    "        # change the tag to index\n",
    "        dataset[i] = list(dataset[i])\n",
    "        dataset[i][0] = label2index.get(pos, label2index['<UNSEEN>'])\n",
    "        dataset[i] = tuple(dataset[i])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_file(file_path, window_size):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = []\n",
    "\n",
    "        # split into sentences (separated by blank rows)\n",
    "        sentences = f.read().split('\\n\\n')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if sentence == '' or sentence == '\\n':\n",
    "                continue\n",
    "            # add special words of start and end of sentence with special labels (<S> = START, <E> = END)\n",
    "            sentence = '<S>\\n<S>\\n' + sentence + '\\n<E>\\n<E>'\n",
    "            words = sentence.split('\\n')\n",
    "\n",
    "            # go over the words (not including the start and end words)\n",
    "            for i in range(window_size, len(words) - window_size):\n",
    "                # insert to the dataset a tuple of label and 5 words when the label is of the middle word\n",
    "                dataset.append(words[i - window_size: i + window_size + 1])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, train_loss_history, train_accuracy_history, dev_loss_history, dev_accuracy_history, path):\n",
    "    torch.save(model, f'{path}/model.path')\n",
    "    torch.save(train_loss_history, f'{path}/train_loss_history.path')\n",
    "    torch.save(train_accuracy_history, f'{path}/train_accuracy_history.path')\n",
    "    torch.save(dev_loss_history, f'{path}/dev_loss_history.path')\n",
    "    torch.save(dev_accuracy_history, f'{path}/dev_accuracy_history.path')\n",
    "\n",
    "\n",
    "def load_model(model_path, train_loss_history_path, train_accuracy_history_path, dev_loss_history_path, dev_accuracy_history_path):\n",
    "    model = torch.load(model_path)\n",
    "    train_loss_history = torch.load(train_loss_history_path)\n",
    "    train_accuracy_history = torch.load(train_accuracy_history_path)\n",
    "    dev_loss_history = torch.load(dev_loss_history_path)\n",
    "    dev_accuracy_history = torch.load(dev_accuracy_history_path)\n",
    "    return model, train_loss_history, train_accuracy_history, dev_loss_history, dev_accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graphs(train_history, dev_history, n_epochs, plot_title, train_title, dev_title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(plot_title)\n",
    "    x = torch.arange(n_epochs) + 1\n",
    "    ax1.set_title(train_title)\n",
    "    ax1.plot(x, train_history)\n",
    "    ax2.set_title(dev_title)\n",
    "    ax2.plot(x, dev_history)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger1Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_words, hidden_dim, out_dim):\n",
    "        super(Tagger1Model, self).__init__()\n",
    "        self.num_words = num_words\n",
    "        self.embed_size = embed_size\n",
    "        self.embed_layer = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.layer1 = nn.Linear(num_words * embed_size, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, words_idxs):\n",
    "        # get the embedded vectors of each word and concat to a large vector\n",
    "        x = self.embed_layer(words_idxs).view(-1, self.num_words * self.embed_size)\n",
    "\n",
    "        x = torch.tanh(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.softmax(self.layer2(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, dev_set, model,  n_epochs, lr, device, index2word, index2label, is_pos=False):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = StepLR(optimizer, step_size=4,gamma=0.1)\n",
    "    # scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    dev_losses = []\n",
    "    dev_accuracy = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        train_loss = train(model, train_set, optimizer, criterion, device)\n",
    "        _, train_acc = evaluate(model, train_set, criterion, device, index2label, is_pos)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracy.append(train_acc)\n",
    "\n",
    "        dev_loss, accuracy = evaluate(model, dev_set, criterion, device, index2label, is_pos)\n",
    "        dev_losses.append(dev_loss)\n",
    "        dev_accuracy.append(accuracy)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'[{e + 1}/{n_epochs}] train loss: {train_loss}, train accuracy: {train_acc}%,'\n",
    "              f' dev loss: {dev_loss}, dev accuracy: {accuracy}%')\n",
    "\n",
    "    save_model(model, train_losses, train_accuracy, dev_losses, dev_accuracy, '.')\n",
    "\n",
    "    # draw graphs of loss and accuracy history\n",
    "    draw_graphs(train_losses, dev_losses, n_epochs, 'Loss History', 'Train Loss', 'Validation Loss')\n",
    "    draw_graphs(train_accuracy, dev_accuracy, n_epochs, 'Accuracy History', 'Train Accuracy', 'Validation Accuracy')b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, optimizer, criterion, device):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(train_set):\n",
    "        labels_batch, words_batch = data\n",
    "\n",
    "        words_batch = torch.stack(words_batch, dim=1)\n",
    "\n",
    "        words_batch = words_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predict\n",
    "        outputs = model(words_batch)\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), labels_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # backwards step\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_set.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, dev_set, criterion, device, index2label, is_pos):\n",
    "    running_loss = 0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(dev_set):\n",
    "        labels_batch, words_batch = data\n",
    "\n",
    "        words_batch = torch.stack(words_batch, dim=1)\n",
    "\n",
    "        words_batch = words_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        # predict\n",
    "        outputs = model(words_batch)\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), labels_batch)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predictions = torch.argmax(outputs.data, dim=1)\n",
    "\n",
    "        if is_pos:\n",
    "            correct += (predictions == labels_batch).sum().item()\n",
    "            total += labels_batch.size(0)\n",
    "        else:\n",
    "            for prediction, real_label in zip(predictions, labels_batch):\n",
    "                # count how many labels were in this batch\n",
    "                total += 1\n",
    "\n",
    "                # check if the prediction in like the real label\n",
    "                if prediction == real_label:\n",
    "                    # if both are 'O' skip it because there are many 'O's (don't count it)\n",
    "                    if index2label[int(prediction)] == 'O':\n",
    "                        total -= 1\n",
    "                    else:\n",
    "                        # otherwise count the correct results\n",
    "                        correct += 1\n",
    "\n",
    "    return running_loss / len(dev_set.dataset), round(100 * correct / total, 3)\n",
    "\n",
    "\n",
    "def predict(test_set, model, device, words2index, index2label):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predicted_labels = []\n",
    "\n",
    "    for i, data in enumerate(test_set):\n",
    "        words_batch = data\n",
    "        words_batch = torch.stack(words_batch, dim=1)\n",
    "\n",
    "        words_batch = words_batch.to(device)\n",
    "\n",
    "        # predict\n",
    "        outputs = model(words_batch)\n",
    "\n",
    "        # get the index of the label\n",
    "        index = torch.argmax(outputs)\n",
    "\n",
    "        # ge the label from the index\n",
    "        label = index2label[index]\n",
    "\n",
    "        predicted_labels.append(label)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_train_set, word2index, index2word, label2index, index2label = utils.parse_POS('./pos/train', window_size=2)\n",
    "# pos_train_set = utils.convert_dataset_to_index(pos_train_set, word2index, label2index)\n",
    "#\n",
    "# pos_dev_set, _, _, _, _ = utils.parse_POS('./pos/dev', window_size=2)\n",
    "# pos_dev_set = utils.convert_dataset_to_index(pos_dev_set, word2index, label2index)\n",
    "\n",
    "ner_train_set, word2index, index2word, label2index, index2label = utils.parse_NER('./ner/train', window_size=2)\n",
    "ner_train_set = utils.convert_dataset_to_index(ner_train_set, word2index, label2index)\n",
    "\n",
    "ner_dev_set, _, _, _, _ = utils.parse_NER('./ner/dev', window_size=2)\n",
    "ner_dev_set = utils.convert_dataset_to_index(ner_dev_set, word2index, label2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-92471997b907>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mis_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# define model's parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "is_pos = False\n",
    "\n",
    "# define model's parameters\n",
    "vocab_size = len(word2index.keys())\n",
    "embed_size = 50\n",
    "num_words = 5\n",
    "out_dim = len(label2index.keys())\n",
    "\n",
    "if is_pos:\n",
    "    lr = 1e-3\n",
    "    n_epochs = 7\n",
    "    batch_size_train = 32\n",
    "    batch_size_dev = 32\n",
    "    hidden_dim = 150\n",
    "else:\n",
    "    lr = 1e-3\n",
    "    n_epochs = 8\n",
    "    batch_size_train = 32\n",
    "    batch_size_dev = 32\n",
    "    hidden_dim = 150\n",
    "\n",
    "print(f'Run config - is POS: {is_pos}, vocab size: {vocab_size}, embed size: {embed_size}, window size: {num_words},'\n",
    "      f' hidden layer size: {hidden_dim}, labels size: {out_dim}, LR: {lr}, epochs: {n_epochs},'\n",
    "      f' train batch size: {batch_size_train}, dev batch size: {batch_size_dev}')\n",
    "\n",
    "\n",
    "# define train dataloader\n",
    "train_data = DataLoader(ner_train_set, batch_size=batch_size_train, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "# train_data = DataLoader(pos_train_set, batch_size=batch_size_train, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "# define train dataloader\n",
    "dev_data = DataLoader(ner_dev_set, batch_size=batch_size_dev, shuffle=False, drop_last=True, pin_memory=True, num_workers=4)\n",
    "# dev_data = DataLoader(pos_dev_set, batch_size=batch_size_dev, shuffle=False, drop_last=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "model = tagger1.Tagger1Model(vocab_size, embed_size, num_words, hidden_dim, out_dim)\n",
    "\n",
    "tagger1.train_model(train_data, dev_data, model, n_epochs, lr, device, index2word, index2label, is_pos)\n",
    "\n",
    "# path = './pos results part 1'\n",
    "\n",
    "# model, train_loss_history, train_accuracy_history, dev_loss_history, dev_accuracy_history = utils.load_model(\n",
    "#     f'{path}/model.path', f'{path}/train_loss_history.path', f'{path}/train_accuracy_history.path',\n",
    "#     f'{path}/dev_loss_history.path', f'{path}/dev_accuracy_history.path'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
